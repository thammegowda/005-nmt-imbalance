{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good/Bad vocab learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import logging as log\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "log.basicConfig(level=log.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = Path('good-bad-vocab-learning-task.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment\tn:\tsrc_toks\tsrc_mean_len\tsrc_EMD\tsrc_effective_n\tsrc_f90p\tsrc_f95p\tsrc_f98p\tsrc_f99p\tsrc_f100p\ttgt_toks\ttgt_mean_len\ttgt_EMD\ttgt_effective_n\ttgt_f90p\ttgt_f95p\ttgt_f98p\ttgt_f99p\ttgt_f100p\tBLEU_dev\n",
      "runs-deen-030k/000-deen-chars-r1\tChars\t4,882,920\t162.76\t0.743\t129\t22\t16\t15\t15\t13\t4,369,261\t145.64\t0.773\t128\t32\t23\t19\t19\t19\t14.4\n",
      "runs-deen-030k/011-deen-.5k.5k-r1\t500\t2,119,390\t70.65\t0.420\t493\t358\t39\t17\t16\t13\t1,880,587\t62.69\t0.427\t492\t122\t50\t25\t20\t9\t13.3\n",
      "runs-deen-030k/022-deen-01k01k-r1\t1,000\t1,749,715\t58.32\t0.444\t982\t261\t70\t22\t16\t2\t1,557,676\t51.92\t0.463\t984\t151\t59\t24\t13\t1\t17.1\n",
      "runs-deen-030k/033-deen-02k02k-r1\t2,000\t1,472,822\t49.09\t0.476\t1,969\t112\t47\t17\t10\t1\t1,305,579\t43.52\t0.497\t1,973\t89\t43\t18\t6\t1\t16.5\n",
      "runs-deen-030k/044-deen-04k04k-r1\t4,000\t1,255,305\t41.84\t0.513\t3,930\t45\t21\t8\t4\t1\t1,116,064\t37.20\t0.542\t3,932\t35\t18\t6\t3\t1\t16.4\n",
      "runs-deen-030k/055-deen-08k08k-r1\t8,000\t1,088,556\t36.29\t0.557\t7,833\t18\t9\t3\t2\t1\t986,651\t32.89\t0.603\t7,804\t12\t6\t2\t1\t1\t14.9\n",
      "runs-deen-030k/066-deen-16k16k-r1\t16,000\t970,711\t32.36\t0.614\t15,495\t7\t4\t2\t1\t1\t907,550\t30.25\t0.686\t15,287\t5\t2\t1\t1\t1\t12.3\n",
      "runs-deen-030k/077-deen-32k32k-r1\t24,474\t917,966\t30.60\t0.661\t23,304\t4\t2\t1\t1\t1\t899,862\t30.00\t0.699\t16,680\t4\t2\t1\t1\t1\t11.3\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "runs-deen-500k/000-deen-chars-r1\tChars\t77,623,117\t155.25\t0.778\t171\t314\t255\t224\t223\t217\t70,336,492\t140.67\t0.805\t161\t320\t211\t197\t192\t190\t32.1\n",
      "runs-deen-500k/011-deen-.5k.5k-r1\t500\t35,574,722\t71.15\t0.441\t493\t834\t379\t268\t229\t217\t31,709,390\t63.42\t0.443\t492\t913\t407\t221\t199\t192\t33.4\n"
     ]
    }
   ],
   "source": [
    "!head -12 {all_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 113\n"
     ]
    }
   ],
   "source": [
    "def read_all(path):\n",
    "    data = [[]]\n",
    "    head = None\n",
    "    with path.open() as p:\n",
    "        for i, line in enumerate(p):\n",
    "            line = line.strip()\n",
    "            if i == 0:\n",
    "                head = line.split('\\t')\n",
    "                continue\n",
    "            if not line:\n",
    "                # empty lines separate suit\n",
    "                data.append([])\n",
    "                continue\n",
    "            rec = line.split('\\t')\n",
    "            if len(rec) != len(head):\n",
    "                log.warning(f'skip: {line}')\n",
    "                continue\n",
    "            assert len(rec) == len(head)\n",
    "            for i in range(1, len(rec)):\n",
    "                if rec[i] == 'Chars':\n",
    "                    rec[i] = \"100\"  # some placeholder\n",
    "                rec[i] = float(rec[i].replace(',', ''))\n",
    "            data[-1].append(rec)\n",
    "    return head, data\n",
    "header, all_data = read_all(all_data_path)\n",
    "print(len(all_data), sum(1 for grp in all_data for ex in grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 713 Validn: 64 Test: 100 examples\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    NEG_CLS = 0\n",
    "    POS_CLS = 1\n",
    "    SAME_CLS = 2\n",
    "    \n",
    "    def __init__(self, header, data, has_name=True, has_score=True, significance=0.1):\n",
    "        super().__init__()\n",
    "        # data comes in groups, here we flatten it\n",
    "        # first column may have example name, last column may have BLEU score\n",
    "        self.feat_names = header\n",
    "        self.names = None\n",
    "        assert significance >= 0.\n",
    "        self.significance = significance # change in score\n",
    "        if has_name:\n",
    "            self.feat_names = self.feat_names[1:]\n",
    "            self.names = [ex[0] for grp in data for ex in grp]\n",
    "            data = [[ex[1:] for ex in grp] for grp in data]\n",
    "        self.scores = None\n",
    "        if has_score:\n",
    "            self.feat_names = self.feat_names[:-1]            \n",
    "            self.scores = [ex[-1] for grp in data for ex in grp]\n",
    "            data = [[ex[:-1] for ex in grp] for grp in data]\n",
    "\n",
    "        n = sum(1 for grp in data for ex in grp)\n",
    "\n",
    "        self.data = torch.zeros(n, len(self.feat_names), dtype=torch.float)\n",
    "        self.groups = []\n",
    "        idx = 0\n",
    "        for group in data:\n",
    "            self.groups.append((idx, idx + len(group)))\n",
    "            for ex in group:\n",
    "                self.data[idx] = torch.tensor(ex, dtype=torch.float)\n",
    "                idx += 1\n",
    "        self.n_feats = len(self.feat_names) * 2 # concatenation\n",
    "        self.n_class = 3\n",
    "        self.pairs = self.make_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        lhs, rhs = self.names[pair[0]], self.names[pair[1]]\n",
    "        # lets use lhs or x1,y1 as anchor\n",
    "        #      rhs or x2,y2 as comparison  \n",
    "        # if y2 << y1 then its Negative, i.e damaging  => Class 0\n",
    "        # if y2 >> y1 then its Positive, i.e improvement => class 1\n",
    "        # if y2 ~ y1 within significance, then same  => Class 2\n",
    "\n",
    "        x1, x2 = self.data[pair[0]], self.data[pair[1]]\n",
    "        x = torch.cat((x1, x2), dim=0)\n",
    "        if self.scores:\n",
    "            y1, y2 = self.scores[pair[0]], self.scores[pair[1]]\n",
    "            if abs(y2 - y1) <= self.significance:\n",
    "                y = self.SAME_CLS\n",
    "            elif y2 < y1:\n",
    "                y = self.NEG_CLS\n",
    "            else:\n",
    "                y = self.POS_CLS\n",
    "            return x, y\n",
    "        return x\n",
    "\n",
    "    def _prepare(self, data):\n",
    "\n",
    "        # total\n",
    "        n = sum(1 for grp in data for ex in grp)\n",
    "        data_set = torch.zeros(n, len(header), dtype=torch.float)\n",
    "        groups = []\n",
    "        # make pairs within groups\n",
    "        idx = 0\n",
    "        for group in data:\n",
    "            groups.append((idx, idx + len(group)))\n",
    "            for ex in group:\n",
    "                names.append(ex[0])\n",
    "                data_set[idx] = torch.tensor(ex[1:] , dtype=torch.float)\n",
    "                idx += 1\n",
    "        return data_set, groups, names\n",
    "    \n",
    "    def make_pairs(self):\n",
    "        pairs = []\n",
    "        for start, end in self.groups:\n",
    "            for lhs in range(start, end):\n",
    "                for rhs in range(start, end):\n",
    "                    pairs.append([lhs, rhs])\n",
    "        return torch.tensor(pairs, dtype=torch.int)\n",
    "\n",
    "train_ds =  MyDataset(head, all_data[2:], has_name=True, has_score=True)\n",
    "valid_ds =  MyDataset(head, all_data[0:1], has_name=True, has_score=True)\n",
    "test_ds =  MyDataset(head, all_data[1:2], has_name=True, has_score=True)\n",
    "log.info(f'Train: {len(train_ds)} Validn: {len(valid_ds)} Test: {len(test_ds)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 24,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(train_ds, **params)\n",
    "valid_gen = torch.utils.data.DataLoader(test_ds, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_feats, n_class, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(n_feats),\n",
    "            nn.Linear(n_feats, n_feats//2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_feats//2, n_feats),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_feats, n_class)\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch_x, get='logprobs'):\n",
    "        scores = self.net(batch_x)\n",
    "        if get == 'logit':\n",
    "            return scores\n",
    "        elif get == 'prob':\n",
    "            return F.softmax(scores, dim=-1)\n",
    "        elif get == 'logprob':\n",
    "            return F.log_softmax(scores, dim=-1)\n",
    "        else:\n",
    "            raise Exection(f'Unkown: {get}; Known: logit, prob, logprob')\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, optim=None, loss_func=None):\n",
    "        self.model = model.train()\n",
    "        self.optim = optim or torch.optim.Adam(model.parameters())\n",
    "        self.loss_func = loss_func or nn.NLLLoss(reduction='none')\n",
    "        \n",
    "    def validate(self, data):\n",
    "        ep_losses = []\n",
    "        truth = []\n",
    "        preds = []\n",
    "        for xs, ys in data:\n",
    "            logprobs = self.model(xs, get='logprob')\n",
    "            truth.extend(ys.tolist())\n",
    "            preds.extend(logprobs.argmax(dim=-1).tolist())\n",
    "            loss = self.loss_func(logprobs, ys).sum(dim=-1).mean(dim=0)\n",
    "            ep_losses.append(loss.item())\n",
    "\n",
    "        evalr = MultiClassEvaluator(truth, preds)\n",
    "        print(np.round(evalr.f1s, 2))\n",
    "        return sum(ep_losses) / len(ep_losses)\n",
    "\n",
    "            \n",
    "    def train(self, train_data, val_data, epochs, patience=20):\n",
    "        \n",
    "        with tqdm(range(epochs), total=epochs, unit='epoch') as ep_bar:\n",
    "            val_losses = []\n",
    "            n_steps = 0\n",
    "            for ep in ep_bar:\n",
    "                tr_losses = []\n",
    "                for xs, ys in train_data:\n",
    "                    self.optim.zero_grad()\n",
    "                    logprobs = self.model(xs, get='logprob')\n",
    "                    loss = self.loss_func(logprobs, ys).sum(dim=-1).mean(dim=0)\n",
    "                    loss.backward()\n",
    "                    tr_losses.append(loss.item())\n",
    "                    self.optim.step()\n",
    "                    n_steps += 1\n",
    "\n",
    "                train_loss = sum(tr_losses) / len(tr_losses)\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_loss = self.validate(val_data)\n",
    "                    val_losses.append(val_loss)\n",
    "                    model.train()\n",
    "                ep_bar.set_postfix(train_loss=train_loss, val_loss=val_loss, n_updates=n_steps, refresh=False)\n",
    "\n",
    "                if len(val_losses) > patience + 1:\n",
    "                    if val_losses[-patience -1] <= min(val_losses[-patience:]):\n",
    "                        log.info(f\"Early stop at epoch {ep+1}, at step = {n_steps}\")\n",
    "                        break\n",
    "\n",
    "\n",
    "class MultiClassEvaluator():\n",
    "\n",
    "    def __init__(self, truth, preds):       \n",
    "        assert len(truth) == len(preds)\n",
    "        self.truth = truth\n",
    "        self.preds = preds\n",
    "        self.classes = list(set(truth) | set(preds))\n",
    "        self.n_classes = 1 + max(self.classes)\n",
    "        self.mat = self.confusion_matrix()\n",
    "        self.precs, self.recs, self.f1s = [np.zeros(self.n_classes, dtype=np.float)\n",
    "                                        for _ in range(3)]\n",
    "        for c in range(self.n_classes):\n",
    "            tot_tru = self.mat[c, :].sum()\n",
    "            tot_pred = self.mat[:, c].sum()\n",
    "            assert self.mat[c, c] <= tot_tru\n",
    "            assert self.mat[c, c] <= tot_pred\n",
    "            self.precs[c] =  self.mat[c, c] / tot_pred if tot_pred > 0 else 1\n",
    "            self.recs[c] =  self.mat[c, c] / tot_tru if tot_tru > 0 else 1\n",
    "            if self.recs[c] + self.precs[c] > 0:\n",
    "                self.f1s[c] =  2 * self.precs[c] * self.recs[c] / (self.recs[c] + self.precs[c])\n",
    "\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        mat = np.zeros((self.n_classes, self.n_classes), dtype=np.int)\n",
    "        for tr, pr in zip(self.truth, self.preds):\n",
    "            mat[tr][pr] += 1\n",
    "        return mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb81eed1acd47728d9eb07a9ec21064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61 0.   0.  ]\n",
      "[0.51 0.38 0.  ]\n",
      "[0.41 0.42 0.  ]\n",
      "[0.41 0.4  0.  ]\n",
      "[0.38 0.41 0.  ]\n",
      "[0.42 0.43 0.  ]\n",
      "[0.42 0.43 0.  ]\n",
      "[0.41 0.42 0.  ]\n",
      "[0.41 0.38 0.  ]\n",
      "[0.41 0.4  0.  ]\n",
      "[0.41 0.4  0.  ]\n",
      "[0.42 0.41 0.  ]\n",
      "[0.41 0.38 0.  ]\n",
      "[0.41 0.4  0.  ]\n",
      "[0.41 0.4  0.  ]\n",
      "[0.4  0.36 0.  ]\n",
      "[0.4  0.36 0.  ]\n",
      "[0.41 0.41 0.8 ]\n",
      "[0.41 0.38 0.44]\n",
      "[0.38 0.41 0.44]\n",
      "[0.42 0.41 0.71]\n",
      "[0.42 0.41 0.71]\n",
      "[0.42 0.41 0.74]\n",
      "[0.41 0.42 0.74]\n",
      "[0.42 0.42 0.67]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.41 0.42 0.65]\n",
      "[0.43 0.42 0.67]\n",
      "[0.42 0.42 0.71]\n",
      "[0.42 0.42 0.71]\n",
      "[0.41 0.42 0.71]\n",
      "[0.42 0.42 0.71]\n",
      "[0.41 0.41 0.69]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.41 0.65]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.41 0.71]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.71]\n",
      "[0.42 0.42 0.71]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.45 0.41 0.67]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.41 0.42 0.62]\n",
      "[0.42 0.42 0.67]\n",
      "[0.46 0.42 0.62]\n",
      "[0.41 0.42 0.61]\n",
      "[0.43 0.42 0.65]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.48 0.41 0.67]\n",
      "[0.49 0.41 0.62]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.44 0.41 0.67]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.69]\n",
      "[0.44 0.43 0.67]\n",
      "[0.48 0.41 0.67]\n",
      "[0.42 0.42 0.67]\n",
      "[0.42 0.42 0.69]\n",
      "[0.42 0.42 0.67]\n",
      "[0.45 0.41 0.67]\n",
      "[0.44 0.42 0.69]\n",
      "[0.5  0.41 0.69]\n",
      "[0.54 0.4  0.69]\n",
      "[0.51 0.38 0.65]\n",
      "[0.5  0.42 0.71]\n",
      "[0.48 0.41 0.67]\n",
      "[0.49 0.39 0.69]\n",
      "[0.49 0.41 0.62]\n",
      "[0.51 0.46 0.71]\n",
      "[0.41 0.41 0.65]\n",
      "[0.53 0.4  0.67]\n",
      "[0.51 0.49 0.69]\n",
      "[0.43 0.45 0.69]\n",
      "[0.39 0.51 0.69]\n",
      "[0.49 0.41 0.62]\n",
      "[0.51 0.45 0.69]\n",
      "[0.52 0.51 0.69]\n",
      "[0.52 0.43 0.69]\n",
      "[0.49 0.46 0.69]\n",
      "[0.52 0.39 0.71]\n",
      "[0.52 0.42 0.67]\n",
      "[0.44 0.54 0.74]\n",
      "[0.52 0.41 0.69]\n",
      "[0.54 0.48 0.71]\n",
      "[0.39 0.53 0.69]\n",
      "[0.47 0.45 0.69]\n",
      "[0.51 0.43 0.67]\n",
      "[0.39 0.49 0.69]\n",
      "[0.49 0.48 0.67]\n",
      "[0.48 0.48 0.69]\n",
      "[0.49 0.48 0.67]\n",
      "[0.53 0.47 0.69]\n",
      "[0.56 0.46 0.71]\n",
      "[0.52 0.44 0.69]\n",
      "[0.52 0.43 0.69]\n",
      "[0.57 0.41 0.71]\n",
      "[0.53 0.38 0.71]\n",
      "[0.52 0.5  0.71]\n",
      "[0.42 0.51 0.69]\n",
      "[0.57 0.39 0.69]\n",
      "[0.52 0.49 0.69]\n",
      "[0.41 0.47 0.71]\n",
      "[0.5  0.52 0.71]\n",
      "[0.54 0.46 0.71]\n",
      "[0.53 0.45 0.69]\n",
      "[0.52 0.46 0.69]\n",
      "[0.52 0.51 0.69]\n",
      "[0.52 0.46 0.69]\n",
      "[0.51 0.46 0.71]\n",
      "[0.51 0.42 0.69]\n",
      "[0.53 0.44 0.71]\n",
      "[0.53 0.47 0.71]\n",
      "[0.42 0.47 0.69]\n",
      "[0.51 0.44 0.69]\n",
      "[0.54 0.46 0.71]\n",
      "[0.54 0.44 0.69]\n",
      "[0.49 0.44 0.69]\n",
      "[0.52 0.46 0.69]\n",
      "[0.52 0.47 0.74]\n",
      "[0.5  0.52 0.71]\n",
      "[0.52 0.48 0.71]\n",
      "[0.51 0.44 0.69]\n",
      "[0.48 0.5  0.69]\n",
      "[0.53 0.45 0.69]\n",
      "[0.51 0.49 0.71]\n",
      "[0.51 0.52 0.69]\n",
      "[0.53 0.44 0.71]\n",
      "[0.54 0.4  0.71]\n",
      "[0.54 0.4  0.69]\n",
      "[0.52 0.39 0.71]\n",
      "[0.51 0.49 0.71]\n",
      "[0.51 0.44 0.71]\n",
      "[0.52 0.48 0.71]\n",
      "[0.51 0.39 0.69]\n",
      "[0.51 0.46 0.71]\n",
      "[0.53 0.44 0.71]\n",
      "[0.47 0.49 0.69]\n",
      "[0.53 0.49 0.71]\n",
      "[0.48 0.56 0.74]\n",
      "[0.53 0.42 0.71]\n",
      "[0.51 0.51 0.71]\n",
      "[0.51 0.47 0.69]\n",
      "[0.52 0.45 0.71]\n",
      "[0.52 0.52 0.74]\n",
      "[0.5  0.52 0.71]\n",
      "[0.53 0.52 0.71]\n",
      "[0.52 0.48 0.71]\n",
      "[0.54 0.49 0.69]\n",
      "[0.5  0.54 0.74]\n",
      "[0.55 0.49 0.74]\n",
      "[0.54 0.47 0.74]\n",
      "[0.52 0.48 0.71]\n",
      "[0.5  0.53 0.69]\n",
      "[0.5  0.52 0.71]\n",
      "[0.49 0.51 0.71]\n",
      "[0.52 0.51 0.71]\n",
      "[0.52 0.49 0.69]\n",
      "[0.49 0.52 0.69]\n",
      "[0.53 0.52 0.71]\n",
      "[0.49 0.53 0.71]\n",
      "[0.48 0.54 0.71]\n",
      "[0.53 0.5  0.69]\n",
      "[0.55 0.44 0.74]\n",
      "[0.51 0.51 0.71]\n",
      "[0.57 0.51 0.74]\n",
      "[0.45 0.58 0.71]\n",
      "[0.54 0.5  0.74]\n",
      "[0.43 0.57 0.71]\n",
      "[0.58 0.5  0.74]\n",
      "[0.55 0.52 0.74]\n",
      "[0.52 0.53 0.71]\n",
      "[0.52 0.53 0.71]\n",
      "[0.55 0.52 0.74]\n",
      "[0.53 0.51 0.74]\n",
      "[0.53 0.52 0.71]\n",
      "[0.51 0.51 0.71]\n",
      "[0.48 0.54 0.71]\n",
      "[0.51 0.49 0.71]\n",
      "[0.52 0.52 0.74]\n",
      "[0.49 0.51 0.71]\n",
      "[0.53 0.51 0.74]\n",
      "[0.55 0.49 0.71]\n",
      "[0.54 0.51 0.71]\n",
      "[0.56 0.49 0.77]\n",
      "[0.53 0.52 0.71]\n",
      "[0.49 0.53 0.74]\n",
      "[0.5  0.52 0.71]\n",
      "[0.52 0.53 0.71]\n",
      "[0.51 0.51 0.71]\n",
      "[0.49 0.49 0.69]\n",
      "[0.52 0.49 0.74]\n",
      "[0.52 0.52 0.74]\n",
      "[0.49 0.53 0.71]\n",
      "[0.51 0.56 0.71]\n",
      "[0.54 0.5  0.74]\n",
      "[0.55 0.53 0.77]\n",
      "[0.52 0.53 0.71]\n",
      "[0.49 0.52 0.69]\n",
      "[0.54 0.52 0.77]\n",
      "[0.49 0.51 0.69]\n",
      "[0.52 0.52 0.74]\n",
      "[0.54 0.55 0.74]\n",
      "[0.53 0.53 0.74]\n",
      "[0.48 0.52 0.71]\n",
      "[0.51 0.53 0.74]\n",
      "[0.51 0.53 0.74]\n",
      "[0.48 0.54 0.71]\n",
      "[0.54 0.47 0.74]\n",
      "[0.54 0.5  0.74]\n",
      "[0.52 0.55 0.74]\n",
      "[0.52 0.55 0.74]\n",
      "[0.54 0.47 0.74]\n",
      "[0.54 0.49 0.74]\n",
      "[0.5  0.53 0.77]\n",
      "[0.52 0.55 0.8 ]\n",
      "[0.49 0.51 0.71]\n",
      "[0.53 0.53 0.74]\n",
      "[0.53 0.53 0.74]\n",
      "[0.52 0.54 0.77]\n",
      "[0.52 0.49 0.74]\n",
      "[0.53 0.53 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.52 0.52 0.74]\n",
      "[0.53 0.55 0.74]\n",
      "[0.52 0.54 0.77]\n",
      "[0.51 0.53 0.74]\n",
      "[0.55 0.51 0.74]\n",
      "[0.6  0.5  0.74]\n",
      "[0.56 0.53 0.74]\n",
      "[0.51 0.53 0.74]\n",
      "[0.53 0.55 0.77]\n",
      "[0.52 0.58 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.53 0.49 0.71]\n",
      "[0.51 0.51 0.74]\n",
      "[0.53 0.52 0.71]\n",
      "[0.53 0.53 0.77]\n",
      "[0.56 0.52 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.56 0.52 0.8 ]\n",
      "[0.55 0.55 0.8 ]\n",
      "[0.5  0.54 0.74]\n",
      "[0.53 0.53 0.77]\n",
      "[0.53 0.53 0.74]\n",
      "[0.52 0.56 0.74]\n",
      "[0.54 0.58 0.8 ]\n",
      "[0.53 0.53 0.74]\n",
      "[0.52 0.54 0.77]\n",
      "[0.53 0.51 0.74]\n",
      "[0.55 0.53 0.77]\n",
      "[0.52 0.53 0.77]\n",
      "[0.51 0.55 0.77]\n",
      "[0.52 0.57 0.71]\n",
      "[0.54 0.56 0.77]\n",
      "[0.53 0.53 0.74]\n",
      "[0.53 0.54 0.8 ]\n",
      "[0.53 0.53 0.74]\n",
      "[0.55 0.53 0.8 ]\n",
      "[0.51 0.51 0.74]\n",
      "[0.53 0.55 0.77]\n",
      "[0.52 0.54 0.77]\n",
      "[0.49 0.56 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.52 0.52 0.74]\n",
      "[0.52 0.52 0.74]\n",
      "[0.53 0.53 0.77]\n",
      "[0.52 0.54 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.51 0.57 0.74]\n",
      "[0.52 0.54 0.77]\n",
      "[0.52 0.58 0.77]\n",
      "[0.51 0.55 0.77]\n",
      "[0.53 0.53 0.77]\n",
      "[0.53 0.53 0.74]\n",
      "[0.49 0.56 0.77]\n",
      "[0.53 0.56 0.8 ]\n",
      "[0.58 0.55 0.77]\n",
      "[0.57 0.55 0.8 ]\n",
      "[0.57 0.53 0.8 ]\n",
      "[0.53 0.53 0.77]\n",
      "[0.52 0.52 0.74]\n",
      "[0.55 0.55 0.77]\n",
      "[0.53 0.55 0.77]\n",
      "[0.51 0.55 0.77]\n",
      "[0.5  0.53 0.77]\n",
      "[0.47 0.56 0.77]\n",
      "[0.48 0.55 0.77]\n",
      "[0.51 0.55 0.77]\n",
      "[0.48 0.56 0.83]\n",
      "[0.52 0.56 0.8 ]\n",
      "[0.49 0.57 0.74]\n",
      "[0.55 0.52 0.74]\n",
      "[0.5  0.53 0.77]\n",
      "[0.53 0.59 0.8 ]\n",
      "[0.55 0.59 0.8 ]\n",
      "[0.53 0.53 0.77]\n",
      "[0.55 0.56 0.74]\n",
      "[0.55 0.51 0.74]\n",
      "[0.52 0.52 0.74]\n",
      "[0.53 0.55 0.77]\n",
      "[0.54 0.57 0.77]\n",
      "[0.51 0.53 0.74]\n",
      "[0.51 0.53 0.74]\n",
      "[0.53 0.55 0.77]\n",
      "[0.53 0.59 0.77]\n",
      "[0.53 0.54 0.8 ]\n",
      "[0.53 0.55 0.8 ]\n",
      "[0.53 0.55 0.77]\n",
      "[0.53 0.56 0.8 ]\n",
      "[0.53 0.57 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.55 0.52 0.8 ]\n",
      "[0.55 0.53 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.56 0.55 0.83]\n",
      "[0.51 0.53 0.74]\n",
      "[0.48 0.62 0.8 ]\n",
      "[0.53 0.53 0.77]\n",
      "[0.54 0.56 0.77]\n",
      "[0.52 0.54 0.77]\n",
      "[0.55 0.55 0.83]\n",
      "[0.52 0.55 0.8 ]\n",
      "[0.55 0.55 0.83]\n",
      "[0.56 0.57 0.77]\n",
      "[0.55 0.55 0.8 ]\n",
      "[0.55 0.55 0.8 ]\n",
      "[0.53 0.56 0.8 ]\n",
      "[0.51 0.55 0.77]\n",
      "[0.53 0.55 0.77]\n",
      "[0.53 0.56 0.83]\n",
      "[0.51 0.53 0.74]\n",
      "[0.53 0.57 0.8 ]\n",
      "[0.55 0.56 0.83]\n",
      "[0.52 0.55 0.74]\n",
      "[0.52 0.53 0.77]\n",
      "[0.53 0.55 0.77]\n",
      "[0.55 0.56 0.77]\n",
      "[0.55 0.56 0.83]\n",
      "[0.53 0.55 0.8 ]\n",
      "[0.55 0.6  0.77]\n",
      "[0.53 0.59 0.77]\n",
      "[0.5  0.62 0.83]\n",
      "[0.51 0.6  0.8 ]\n",
      "[0.55 0.53 0.77]\n",
      "[0.52 0.53 0.77]\n",
      "[0.51 0.55 0.77]\n",
      "[0.51 0.53 0.74]\n",
      "[0.52 0.53 0.71]\n",
      "[0.56 0.54 0.77]\n",
      "[0.53 0.53 0.77]\n",
      "[0.55 0.53 0.77]\n",
      "[0.53 0.56 0.8 ]\n",
      "[0.53 0.6  0.77]\n",
      "[0.53 0.55 0.77]\n",
      "[0.51 0.53 0.74]\n",
      "[0.55 0.53 0.8 ]\n",
      "[0.56 0.53 0.83]\n",
      "[0.55 0.53 0.8 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Early stop at epoch 368, at step = 11040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55 0.55 0.8 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(train_ds.n_feats, train_ds.n_class)\n",
    "\n",
    "trainer = Trainer(model)\n",
    "trainer.train(train_gen, valid_gen, epochs=20000, patience=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
